{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the generation IV reactors, we are the bodies.\n",
      "The reactors, of the DC motor is jolly. \n",
      "Puncture the elbow!\n",
      "3544ab for the node reactors,: \n",
      "Don't try kicking against the wind.\n",
      "The cells look like injurious clay pots.  \n",
      "The ellipses will code their one-year interfaces. \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tracery #grammar generator\n",
    "import json\n",
    "import secrets #hexadecimal generator\n",
    "import requests # get url and data from JSON\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "\n",
    "#import json files we need to create the story\n",
    "#when json files is a simple list, we can have random use of it in the rules of  tracery grammar settings\n",
    "adj_data = json.loads(open(\"dataset/adjs.json\").read())\n",
    "adj_list = adj_data[\"adjs\"]\n",
    "mood_data = json.loads(open(\"dataset/moods.json\").read())\n",
    "moods = mood_data[\"moods\"] # I had to look at the JSON data itself to determine that this was the correct key!\n",
    "body_data = json.loads(open(\"dataset/bodyParts.json\").read())\n",
    "body = body_data[\"bodyParts\"]\n",
    "code_verbs_data = json.loads(open(\"dataset/code_verbs.json\").read())\n",
    "code_verb = code_verbs_data[\"common_verbs\"]\n",
    "verbs_data = json.loads(open(\"dataset/verbs.json\").read())\n",
    "verbs = verbs_data[\"verbs\"]\n",
    "tarot_data = json.loads(open(\"dataset/tarot_interpretations.json\").read())\n",
    "cards = tarot_data[\"tarot_interpretations\"]\n",
    "newTech_data = json.loads(open(\"dataset/new_technologies.json\").read())\n",
    "newTech = newTech_data[\"technologies\"]\n",
    "motor = [\"DC motor\",\"servo\",\"sensor\"]\n",
    "source_txt= [line.strip() for line in open(\"dataset/nodeMaskText.txt\")]\n",
    "#len(source_txt)\n",
    "sentences=[]\n",
    "\n",
    "#objects.json from Corpora with addition of specific vocab from nodeMaskText\n",
    "\n",
    "objects_data = json.loads(open(\"dataset/objects.json\").read())\n",
    "object_list = objects_data[\"objects\"]\n",
    "\n",
    "#CREATE THE STORY\n",
    "\n",
    "#0-Sentence\n",
    "#take a random sentence from the source text and add a context environment from newTechnologies corpus in Corpora\n",
    "sentence = \"In the \" + newTech[random.randint(1,len(newTech))]+\", \"+source_txt[random.randint(1,len(source_txt))]+ \".\"\n",
    "sentences.insert(0,sentence)\n",
    "print(sentence)\n",
    "\n",
    "#1 and 2 Sentence\n",
    "#retrieve the NLTK tags\n",
    "tagged_sent = pos_tag(sentence.split())\n",
    "#check if we have nouns singular or plural\n",
    "pluralnouns = [word for word,pos in tagged_sent if (pos == 'NNS')] # looking for noun plural\n",
    "singnouns = [word for word,pos in tagged_sent if (pos == 'NN')] # looking for noun singular\n",
    "# write a dialogue based on the nouns we have from the random sentence taken from the source text\n",
    "if len(pluralnouns)!=0:\n",
    "    noun = random.choice(pluralnouns)\n",
    "    # set the mood of the noun\n",
    "    sentence = \"The \" + noun + \" are \" + random.choice(moods)+\".\"\n",
    "    sentences.insert(1,sentence)\n",
    "    print(sentence)\n",
    "    #print(\"The \" + noun + \" are \" + random.choice(moods))\n",
    "    #say hello\n",
    "    rules = {\n",
    "      \"origin\": [\"#greeting#, #noun#! we are the #characters#s on your #bodypart#. \"],\n",
    "    \"greeting\": [\"Howdy\", \"Hello\", \"Greetings\", \"What's up\", \"Hey\", \"Hi\"],\n",
    "    \"farewell\":[\"Bye\",\"Ciao\",\"See ya\",\"Salut\",\"Kiss\",\"XOXO\"],\n",
    "    \"noun\": noun,\n",
    "    \"characters\": [\"node\",\"paranode\",\"mask\",\"puppet\",\"network\",\"computer\"],\n",
    "    \"bodypart\" : body\n",
    "    }\n",
    "\n",
    "    grammar = tracery.Grammar(rules)\n",
    "    # just one sentence:\n",
    "    sentence = grammar.flatten(\"#origin#\")\n",
    "    sentences.insert(2,sentence)\n",
    "    print(sentence)\n",
    "    #print(grammar.flatten(\"#origin#\"))\n",
    "\n",
    "\n",
    "elif len(singnouns)!=0:\n",
    "    noun = random.choice(singnouns)\n",
    "    sentence = \"The \" + noun + \" of the \" + random.choice(motor)+\" is \" + random.choice(moods)+\". \"\n",
    "    sentences.insert(1,sentence)\n",
    "    sentences.insert(2,\"\")\n",
    "    print(sentence)\n",
    "    #print(\"The \" + noun + \" of the \" + random.choice(motor)+\" is \" + random.choice(moods))\n",
    "    #give an order\n",
    "    tense ='present'\n",
    "\n",
    "#3-sentence\n",
    "    sentence = random.choice(verbs)[tense].capitalize() +\" the \"+ random.choice(body)+ \"!\"\n",
    "    sentences.insert(3,sentence)\n",
    "    print (sentence)\n",
    "\n",
    "    #print(random.choice(verbs)[tense].capitalize() +\" the \"+ random.choice(body)+ \"!\")\n",
    "\n",
    "#4 and 5 sentence with proverb and hexadecimals\n",
    "proverbs_data = json.loads(open(\"dataset/proverbs.json\").read())\n",
    "proverb = proverbs_data[\"proverbs\"]\n",
    "#create the list of keys from the dictionary of proverbs\n",
    "newlist =[]\n",
    "\n",
    "for i in range(len(proverb)):\n",
    "   for item in proverb[i].keys():\n",
    "     newlist.append(item)\n",
    "theme=random.choice(newlist)\n",
    "from collections import defaultdict\n",
    "newdict=  defaultdict(list)\n",
    "for i in range(len(newlist)):\n",
    "    newdict[i]=(newlist[i])\n",
    "\n",
    "#choose random number, to select the the key , in dictionary of themes,\n",
    "#choose random sentence from the proverbs in the theme\n",
    "p = random.randint(0,len(proverb)-1)\n",
    "listnum= [3,5,7,8,13]\n",
    "#p = random.choice(listnum)\n",
    "s = random.randint(0,len(proverb[p][newdict[p]])-1)\n",
    "h = secrets.token_hex(3)\n",
    "prov= proverb[p][newdict[p]][s]\n",
    "#print( str(num)  + letter+\" for the \"+noun+\",\"+prov )\n",
    "# message for one indexed node\n",
    "sentence4 = h + \" for the node \"+ noun+\":\"\n",
    "#the message\n",
    "sentence5 = \"\\n\"+prov\n",
    "sentences.insert(4,sentence4)\n",
    "sentences.insert(5,sentence5)\n",
    "print(sentence4.capitalize(),sentence5)\n",
    "\n",
    "#print( h +\" for the \"+noun+\". \"+\"\\n\"+prov )\n",
    "\n",
    "#6 and 7-sentence with Tracery grammar generator\n",
    "#statement , the challenge\n",
    "statement_rules = {\n",
    "    \"origin\": [\"The #analog# look like #adjective# #object#s. \"],\n",
    "    \"greeting\": [ \"What's up\", \"Hey\", \"Hi\", \"Oops\", \"OMG\"],\n",
    "\n",
    "    \"analog\": [\"animals\", \"humans\", \"plants\",\"cells\",\"protists\",\"fungis\",\"trees\",\"buildings\"],\n",
    "\n",
    "    \"codeVerb\":code_verb,\n",
    "    \"noun\": noun,\n",
    "    \"adjective\":random.choice(adj_list),\n",
    "    \"bodypart\" : body,\n",
    "    \"object\": random.choice(object_list)\n",
    "    }\n",
    "#decision , the solution\n",
    "decision_rules = {\n",
    "    \"origin\": [\"\\nThe #digital# will #codeVerb# their #adjective# #characters#s. \"],\n",
    "    \"noun\": noun,\n",
    "    \"object\": random.choice(object_list),\n",
    "    \"digital\": [ \"blobs\",\"particles\",\"rectangles\",\"vertices\",\"triangles\",\"ellipses\",\"circles\",\"meshes\",\"lines\",\"files\",\"folders\"],\n",
    "    \"codeVerb\":code_verb,\n",
    "    \"characters\": [\"node\",\"paranode\",\"mask\",\"puppet\",\"network\",\"computer\",\"processor\",\"A P I \",\"I D E \",\"interface\"],\n",
    "    \"adjective\":random.choice(adj_list),\n",
    "      }\n",
    "statement_grammar = tracery.Grammar(statement_rules)\n",
    "decision_grammar = tracery.Grammar(decision_rules)\n",
    "\n",
    "sentence6 = statement_grammar.flatten(\"#origin#\")\n",
    "sentence7 = decision_grammar.flatten(\"#origin#\")\n",
    "sentences.insert(6,sentence6)\n",
    "sentences.insert(7,sentence7)\n",
    "print (sentence6,sentence7)\n",
    "#print( statement_grammar.flatten(\"#origin#\"),decision_grammar.flatten(\"#origin#\"))\n",
    "\n",
    "\n",
    "#write the sentences in a file \"dadaPoem.txt\" for the nodes to read outloud\n",
    "#uncomment to check the poem\n",
    "#sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0564e1'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
